{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('base': conda)",
      "language": "python",
      "name": "python37564bitbasecondab1f9fe92a44e4b7e9e8871da5b5c8925"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "PyTorch_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jishma-shareena/google_crash_onML/blob/master/PyTorch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsMQSQzoRvba",
        "colab_type": "text"
      },
      "source": [
        "### Autograd : Automatic Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1W_YLaKRvbo",
        "colab_type": "text"
      },
      "source": [
        "- This is central to the implementation of Neural Networks.\n",
        "- Define-by-run framework\n",
        "- Autograd package provides automatic differentiation of all tensors in the program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-nuKymPRvbz",
        "colab_type": "text"
      },
      "source": [
        "##### **requires_grad**\n",
        "   - This set to True on a tensor will ask the system to keep track of the computations\n",
        "   \n",
        "##### **grad_fn**\n",
        "   - Every tensor has a grad_fn which refers to the function which created it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wW0w-BlRvb7",
        "colab_type": "code",
        "colab": {},
        "outputId": "9edcb2db-8ad9-4c2c-a406-d8f5678db702"
      },
      "source": [
        "import torch as tch\n",
        "\n",
        "x = tch.randn(2,2,requires_grad=True)\n",
        "y = x + 3\n",
        "print(x.grad_fn)#None as the tensor is user defined\n",
        "print(x,y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "tensor([[ 0.1153,  1.2603],\n",
            "        [-0.5387,  0.6195]], requires_grad=True) tensor([[3.1153, 4.2603],\n",
            "        [2.4613, 3.6195]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgqGjJ8XRvcX",
        "colab_type": "code",
        "colab": {},
        "outputId": "fdcc85c0-8633-463b-e2bc-e5f0ebe28d00"
      },
      "source": [
        "z = (y*y).sum()+1\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(25.6687, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoBPOV-VRvcr",
        "colab_type": "text"
      },
      "source": [
        "- **To compute the gradient you call *.backward()* on the tensor.**\n",
        "- **To get the gradient there is a *grad* field associated with each tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUSyzdSsRvcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CugasAYeRvc8",
        "colab_type": "code",
        "colab": {},
        "outputId": "0178f104-e5eb-41be-ecc5-68d5c02b26db"
      },
      "source": [
        "print(x.grad)#gradient of the z wrt to x => dz/dx\n",
        "print(y.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3.5992, 2.9023],\n",
            "        [6.6468, 5.7548]])\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h4BcrtjRvdL",
        "colab_type": "text"
      },
      "source": [
        "##### A more complex example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zysqAlXzRvdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = tch.tensor(3.0,requires_grad=True)\n",
        "x = z\n",
        "\n",
        "while x < 1000:\n",
        "    x = x*2\n",
        "\n",
        "y = x\n",
        "\n",
        "y.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3BOUPlMRvdd",
        "colab_type": "code",
        "colab": {},
        "outputId": "b741c37a-53f8-4ad2-8d8d-4d64bc14f867"
      },
      "source": [
        "print(z.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(512.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3QOvgWaRvdq",
        "colab_type": "text"
      },
      "source": [
        "- You can stop autograd from tracking the history by wrapping the code in ***with torch.no_grad***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j957TEvtRvdt",
        "colab_type": "code",
        "colab": {},
        "outputId": "ca12462f-f8e1-48a4-b2ac-dad79ad6fc6a"
      },
      "source": [
        "print((x+2).requires_grad)\n",
        "with tch.no_grad():\n",
        "    print((x+2).requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUWahDFuRvd6",
        "colab_type": "text"
      },
      "source": [
        "- You can also create a new tensor with no tracking from one requiring gradient by using .detach function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh3JwBffRvd9",
        "colab_type": "code",
        "colab": {},
        "outputId": "c095a093-15ee-4220-abda-6b7a6b67508b"
      },
      "source": [
        "r = (x+3).detach()\n",
        "print(r.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}